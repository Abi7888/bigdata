-- Simple Hive Program

-- before entering Hive, can load data from s3 source with
-- hadoop fs -cp s3://bigdatabucket123/wdi_extract.csv /user/
-- or upload via interface to /tmp

-- specify variable names

CREATE TABLE wdi (Country STRING,Survival_to_65_female INT,Survival_to_65_male INT,Pop_0_14 INT,Pop_15_64 INT,Pop_65_up INT)
    ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;

LOAD DATA INPATH '/tmp/wdi_extract.csv'
    OVERWRITE INTO TABLE wdi;

-- queries

SELECT Country FROM wdi;

SELECT Country, Pop_65_up FROM wdi
    SORT BY Pop_65_up DESC; 

SELECT Country, Pop_65_up, Survival_to_65_female FROM wdi
    WHERE Survival_to_65_female>90
    SORT BY Pop_65_up DESC; 
